# Enhanced Audio Event Detection System

## ğŸ¯ Overview

Production-ready audio event detection system using **ALBERT-base-v2** transformer with mel-spectrogram preprocessing. Detects audio events from background noise with comprehensive training infrastructure and evaluation tools.

## âœ… System Status: **FULLY OPERATIONAL**

- **Model**: ALBERT-base-v2 with mel-spectrogram features (128-dim)
- **Data**: 838 event samples + 7,464 background samples  
- **Validation**: 4/4 tests passing âœ…
- **Ready**: Training, evaluation, and deployment ready

## ğŸš€ Quick Start

### Installation
```bash
pip install -r requirements.txt
```

### Training
```bash
# Simple training (recommended)
python3 training_simple.py

# Advanced training with full features
python3 training_enhanced.py --event-dir dataset_current/training --noise-dir Background

# Custom configuration
python3 training_enhanced.py --config custom_config.json
```

### Validation
```bash
python3 validate_system.py  # Verify system integrity
```

## ğŸ“ Core Components

```
ğŸ“¦ Enhanced Audio Event Detection
â”œâ”€â”€ ğŸ¤– AI/ML Core
â”‚   â”œâ”€â”€ model.py              # ALBERT-based audio classifier
â”‚   â”œâ”€â”€ dataset.py            # Audio preprocessing pipeline
â”‚   â”œâ”€â”€ utils.py              # Metrics and evaluation tools
â”‚   â””â”€â”€ config.py             # Configuration management
â”‚
â”œâ”€â”€ ğŸ“ Training Scripts
â”‚   â”œâ”€â”€ training_simple.py    # Quick start training
â”‚   â”œâ”€â”€ training_enhanced.py  # Full-featured training
â”‚   â””â”€â”€ validate_system.py    # System validation
â”‚
â”œâ”€â”€ ğŸ“Š Data & Config
â”‚   â”œâ”€â”€ config_default.json   # Default settings
â”‚   â”œâ”€â”€ requirements.txt      # Dependencies
â”‚   â”œâ”€â”€ dataset_current/      # Event samples (838 files)
â”‚   â””â”€â”€ Background/           # Noise samples (7,464 files)
â”‚
â””â”€â”€ ğŸ“– Documentation
    â”œâ”€â”€ SYSTEM_SUMMARY.md     # Quick overview
    â””â”€â”€ README_ENHANCED.md    # This file
```

## âš™ï¸ Key Features

- **ALBERT-base-v2**: Efficient transformer architecture
- **Mel-spectrograms**: Advanced audio feature extraction
- **Data augmentation**: Gain, noise, pitch shift
- **Early stopping**: Prevent overfitting
- **Comprehensive metrics**: Accuracy, precision, recall, F1, ROC-AUC
- **GPU support**: CUDA optimization when available

## ğŸ”§ Configuration

The system uses JSON-based configuration. Key sections:

```json
{
  "audio": {
    "sample_rate": 16000,
    "max_duration": 10.0,
    "n_mels": 128
  },
  "model": {
    "model_name": "albert-base-v2",
    "num_classes": 2
  },
  "training": {
    "num_train_epochs": 20,
    "learning_rate": 2e-5,
    "per_device_train_batch_size": 16
  }
}
```

## ğŸ“ˆ Model Architecture

```
Audio Input (16kHz WAV)
    â†“
Mel-Spectrogram Extraction (128-dim)
    â†“
Feature Projection Layer
    â†“
ALBERT Transformer (albert-base-v2)
    â†“
Classification Head
    â†“
Event/No-Event Prediction
```

## ğŸ›ï¸ Advanced Features

### Data Processing
- **Mel-spectrograms**: 128 mel-frequency bins
- **Audio augmentation**: Gain, noise, pitch variations
- **Caching**: Preprocessed feature caching for speed
- **Balanced sampling**: Handle imbalanced datasets

### Training Infrastructure
- **Early stopping**: Configurable patience and criteria
- **Model checkpointing**: Save best performing models
- **Metrics tracking**: Comprehensive evaluation metrics
- **Logging**: Detailed training progress logs

### Evaluation Tools
- **Confusion matrices**: Visual performance analysis
- **Per-class metrics**: Detailed breakdown by class
- **Training visualization**: Loss and metric plots
- **Model comparison**: Compare multiple trained models

## ğŸ” Usage Examples

### Custom Training
```python
from config import ExperimentConfig
from training_enhanced import train_model

config = ExperimentConfig.from_json("config_default.json")
config.training.learning_rate = 1e-5
results = train_model(config)
```

### Model Evaluation
```python
from utils import evaluate_model_comprehensive

metrics = evaluate_model_comprehensive(
    model=model,
    eval_dataloader=dataloader,
    device=device,
    output_dir="results/"
)
```

## ğŸ› Troubleshooting

### Common Issues
1. **Memory errors**: Reduce batch size in config
2. **Audio format**: Ensure 16kHz WAV files
3. **CUDA issues**: Check GPU availability with `python3 validate_system.py`

### Performance Tips
- Use GPU when available for faster training
- Enable caching for repeated runs
- Adjust batch size based on available memory
- Use early stopping to prevent overfitting

## ğŸ“Š System Validation

Run validation to ensure everything works:
```bash
python3 validate_system.py
```

Expected output:
```
âœ… Module Imports: PASSED
âœ… Configuration System: PASSED  
âœ… Model Creation: PASSED
âœ… Audio Processing: PASSED
Overall: 4/4 tests passed
```

## ğŸ”„ Migration Notes

Enhanced system improvements over original:
- **Model**: wav2vec2 â†’ ALBERT-base-v2
- **Features**: Basic audio â†’ Mel-spectrograms
- **Training**: Simple loop â†’ Professional pipeline
- **Config**: Hardcoded â†’ JSON-based system
- **Evaluation**: Basic â†’ Comprehensive metrics

## ğŸ“š Key Dependencies

- PyTorch + transformers (Hugging Face)
- librosa (audio processing)
- sklearn (metrics)
- matplotlib + seaborn (visualization)
- See `requirements.txt` for complete list

---

**Status**: Production ready â€¢ **Tests**: 4/4 passing â€¢ **Last updated**: June 2025
